"""Answer generation module using streaming LLM responses.

This module handles the core answer generation logic, streaming responses
from the LLM and processing them for real-time display.
"""

import asyncio
import time
from typing import AsyncGenerator, List, Dict, Any, Optional

from backend.app.core.singletons import LLMClientSingleton, LoggerSingleton
from backend.app.core.config import get_config
from backend.app.answer.prompt import (
    build_system_prompt, 
    build_user_prompt, 
    process_quotes,
    build_context_summary,
    extract_title_and_content
)

_cfg = get_config()
_logger = LoggerSingleton().get()


async def generate_answer(
    query: str, 
    context_items: List[str],
    model: Optional[str] = None,
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    context_window: Optional[int] = None
) -> AsyncGenerator[str, None]:
    """Generate a streaming answer from the given query and context.
    
    Args:
        query: The user's question
        context_items: List of retrieved context strings
        model: Optional model override
        temperature: Optional temperature override
        max_tokens: Optional max_tokens override
        context_window: Optional context_window override
        
    Yields:
        String tokens as they are generated by the LLM
    """
    start_time = time.time()
    
    _logger.info(f"Starting answer generation for query: {query[:100]}...")
    _logger.info(f"Context summary: {build_context_summary(context_items)}")
    
    # Build messages for the LLM
    system_msg = {"role": "system", "content": build_system_prompt()}
    user_msg = {"role": "user", "content": build_user_prompt(query, context_items)}
    
    # Get LLM client
    client = LLMClientSingleton()
    
    # Track the complete answer as we stream
    answer_md = ""
    token_count = 0
    
    # Prepare LLM parameters
    kwargs = {}
    if model:
        kwargs["model"] = model
    if temperature:
        kwargs["temperature"] = temperature
    if max_tokens:
        kwargs["max_tokens"] = max_tokens
    if context_window:
        kwargs["context_window"] = context_window
    
    try:
        # Stream the response
        async for delta in client.create_answer_generation_chat(
            messages=[system_msg, user_msg],
            **kwargs
        ):
            answer_md += delta
            token_count += 1
            yield delta
              # Post-process the complete answer
        final_answer = process_quotes(answer_md, context_items)
        
        # Log completion stats
        duration = time.time() - start_time
        _logger.info(f"Answer generation completed in {duration:.2f}s, {token_count} tokens")
        _logger.info(f"Final answer length: {len(final_answer)} characters")
        
        # Yield any additional content from post-processing
        if len(final_answer) > len(answer_md):
            additional_content = final_answer[len(answer_md):]
            yield additional_content
            
    except Exception as e:
        _logger.error(f"Error during answer generation: {e}")
        error_msg = f"\n\n*Error generating answer: {str(e)}*"
        yield error_msg
        raise


async def generate_answer_complete(query: str, context_items: List[str]) -> str:
    """Generate a complete answer (non-streaming) for cases where we need the full text.
    
    Args:
        query: The user's question
        context_items: List of retrieved context strings
        
    Returns:
        The complete generated answer
    """
    complete_answer = ""
    async for token in generate_answer(query, context_items):
        complete_answer += token
    
    return complete_answer
