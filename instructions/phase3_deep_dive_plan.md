# **SocioGraph RebuildÂ â€” PhaseÂ 3Â Deepâ€‘Dive Plan**

> **Objective:** Transform uploaded PDFs into **semantic chunks**, embed them into the Chroma vector store, and populate the SQLite knowledgeâ€‘graph with deduplicated entities and relationships. This phase provides the fuel that every later query relies on.

---

## ğŸ¯ Outcomes

| ID | Outcome | AcceptanceÂ Criteria |
|----|---------|---------------------|
| Oâ€‘3.1 | **Reset** endpoint completely clears state. | After `POST /reset` the folders `vector_store/`, `input/`, `saved/`, and file `graph.db` no longer exist. |
| Oâ€‘3.2 | **Upload** endpoint stores PDFs in `input/` with original filenames (minus extension). | Uploading `doc.pdf` results in `input/doc.pdf`. |
| Oâ€‘3.3 | `ingest.process_all()` converts every page into semantic chunks (â‰ˆÂ 1â€‘3 paragraphs each). | Average chunk length 40â€“120â€¯tokens; no chunk crosses section boundaries. |
| Oâ€‘3.4 | All chunks embedded and persisted to Chroma collection `chunks`. | `collection.count()` equals total chunks after ingest. |
| Oâ€‘3.5 | Entities & relations extracted and written to `graph.db`, reusing existing IDs when cosineâ€¯â‰¥â€¯0.90. | No duplicate entity rows for surface strings that differ only in whitespace/case. |
| Oâ€‘3.6 | Endâ€‘toâ€‘end processing of a 5â€‘page PDF completes in **<â€¯30â€¯s** on a CPU laptop. | Timer test passes. |
| Oâ€‘3.7 | Progress stream sends JSON updates (`phase`, `percent`) to the client. | UI displays realâ€‘time progress bar. |

---

## ğŸ“‹ Pipeline Overview îˆ€fileciteîˆ‚turn3file0îˆ

```
PDF â†’ text pages
      â†“
SemanticSplitterNodeParser
      â†“
       chunks â”€â”€â–º Sentenceâ€‘Transformer Embedding â”€â”€â–º Chroma ('chunks')
      â†“
LLM Entity/Relation JSON (graph_prompts.py)
      â†“
Dedup via sqliteâ€‘vss ANN (sim â‰¥â€¯0.90)
      â†“
SQLite tables: entity, relation
```

---

## âš™ï¸ Prerequisites

* **PhasesÂ 0â€‘2 complete** (env + Config + singletons).  
* Extra packages:

```bash
pip install pypdf llama-index pdfminer.six
```

* `OPENROUTER_API_KEY` exported.

---

## ğŸ› ï¸ Stepâ€‘byâ€‘Step Implementation

### 1Â Â Endpoints Skeleton (`backend/app/api/ingest.py`)

```python
from fastapi import APIRouter, UploadFile, BackgroundTasks
from backend.app.ingest.pipeline import process_all, reset_corpus

router = APIRouter(tags=["ingest"])

@router.post("/reset")
async def reset():
    reset_corpus()
    return {"status": "corpus cleared"}

@router.post("/upload")
async def upload(file: UploadFile, tasks: BackgroundTasks):
    dest = (get_config().INPUT_DIR / file.filename).with_suffix(".pdf")
    dest.write_bytes(await file.read())
    tasks.add_task(process_all)   # run async
    return {"status": "uploaded", "file": dest.name}
```

### 2Â Â Reset Helper (`backend/app/ingest/reset.py`)

```python
def reset_corpus():
    cfg = get_config()
    for path in [cfg.VECTOR_DIR, cfg.INPUT_DIR, cfg.SAVED_DIR]:
        shutil.rmtree(path, ignore_errors=True)
        path.mkdir(exist_ok=True)
    Path(cfg.GRAPH_DB).unlink(missing_ok=True)
```

### 3Â Â PDF Loader (`backend/app/ingest/loader.py`)

Use **pypdf** to extract page text:

```python
from pypdf import PdfReader
def load_pages(pdf_path: Path) -> list[str]:
    reader = PdfReader(pdf_path)
    return [p.extract_text() or "" for p in reader.pages]
```

### 4Â Â Semantic Chunking îˆ€fileciteîˆ‚turn3file6îˆ

```python
from llama_index.node_parser import SemanticSplitterNodeParser
from backend.app.core import embed

_SPLITTER = SemanticSplitterNodeParser(
    buffer_size=2,                          # per guideline
    breakpoint_percentile_threshold=60,     # aggressive for PDFs
    embed_model=None,                       # we supply preâ€‘embedded text
)

def chunk_page(text: str) -> list[str]:
    nodes = _SPLITTER.split_text(text)
    return [n.get_content().strip() for n in nodes if n.get_content().strip()]
```

*Average chunk targetÂ â‰ˆÂ 80â€¯tokens (tuned via `breakpoint_percentile_threshold`).*

### 5Â Â Vector Index

```python
from backend.app.core import ChromaSingleton, embed

def add_chunks(chunks: list[str], source_file: str):
    vecs = embed(chunks)
    ids  = [f"{source_file}:{i}" for i in range(len(chunks))]
    meta = [{"text": c, "file": source_file} for c in chunks]
    ChromaSingleton().add_texts(chunks, embeddings=vecs, metadatas=meta, ids=ids)
```

### 6Â Â Entity & Relation Extraction îˆ€fileciteîˆ‚turn3file8îˆ

```python
import json, asyncio
from backend.app.core import LLMClientSingleton, SQLiteSingleton, embed
import backend.app.prompts.graph_prompts as gp

SYSTEM = gp.SYSTEM_PROMPT
USER_TPL = gp.USER_PROMPT_TEMPLATE

async def extract_entities(chunks: list[str]):
    client = LLMClientSingleton()
    for chunk in chunks:
        prompt = [
            {"role": "system", "content": SYSTEM},
            {"role": "user",   "content": USER_TPL.format(text=chunk)}
        ]
        json_line = ""
        async for delta in client.stream_chat(prompt, max_tokens=1000, temperature=0.3):
            json_line += delta
        rows = json.loads(json_line)
        _insert_graph_rows(rows)

def _insert_graph_rows(rows):
    con = SQLiteSingleton()
    for obj in rows:
        head_id = _get_or_insert_entity(obj["head"], obj["head_type"])
        tail_id = _get_or_insert_entity(obj["tail"], obj["tail_type"])
        con.execute(
            "INSERT OR IGNORE INTO relation(head_id, tail_id, rel_type) VALUES(?,?,?)",
            (head_id, tail_id, obj["relation"]),
        )
    con.commit()
```

#### Dedup Logic

```python
def _get_or_insert_entity(surface: str, typ: str) -> int:
    con = SQLiteSingleton()
    cur = con.execute("SELECT id, embedding FROM entity WHERE type=?",
                      (typ,))
    vec_new = embed(surface)[0]
    for row in cur.fetchall():
        if cosine(row["embedding"], vec_new) >= get_config().ENTITY_SIM:   # 0.90
            return row["id"]
    cur = con.execute(
        "INSERT INTO entity(surface, type, embedding) VALUES(?,?,?)",
        (surface, typ, vec_new)
    )
    return cur.lastrowid
```

### 7Â Â Batch Orchestrator (`pipeline.py`)

```python
def process_all():
    for pdf in get_config().INPUT_DIR.glob("*.pdf"):
        pages = load_pages(pdf)
        chunks = sum((chunk_page(p) for p in pages), [])
        add_chunks(chunks, pdf.stem)
        asyncio.run(extract_entities(chunks))
```

Add logging + `yield {"phase":"..."}` to feed the UI.

---

## ğŸ“ Validation Script

```bash
python - <<'PY'
from backend.app.ingest.pipeline import process_all
process_all()
from backend.app.core import ChromaSingleton, SQLiteSingleton
print("Chunks:", ChromaSingleton()._collection.count())
print("Entities:", SQLiteSingleton().execute("SELECT COUNT(*) FROM entity").fetchone()[0])
PY
```

Expect **nonâ€‘zero** counts.

---

## ğŸ•‘ Estimated Effort

| Task | Time (min) |
|------|------------|
| Endpoint scaffolding | 10 |
| Reset + loader | 5 |
| Semantic chunking tuning | 15 |
| Vector insert & tests | 8 |
| Entity extraction & dedup | 20 |
| Progress & validation | 5 |
| **Total** | **~1â€¯hr** |

---

## ğŸš‘ Troubleshooting & Tips

| Symptom | Likely Cause | Fix |
|---------|--------------|-----|
| Chunks extremely short/long | Splitter thresholds off | Adjust `breakpoint_percentile_threshold` (50â€“70) |
| `sqlite3.InterfaceError: Error binding parameter` | Attempting to pass list as BLOB | Serialize embeddings with `array('f')` bytes |
| LLM times out on big chunk | Chunk exceeds 3â€¯k tokens | Limit chunk length preâ€‘prompt |
| Duplicate entity rows | Cosine threshold too high | Lower to 0.88 or normalise embeddings |

---

## ğŸ“ Deliverables

1. **`backend/app/ingest/`** package: `loader.py`, `chunker.py`, `pipeline.py`, `reset.py`  
2. API router **`ingest.py`** and registration in `main.py`  
3. Populated `vector_store/` and `graph.db` after sample run  
4. Git tag **`phaseâ€‘3`**

---

_When the validation script prints nonâ€‘zero counts and the UI displays realâ€‘time progress for `/upload`, **PhaseÂ 3 is complete**. Proceed to **PhaseÂ 4 â€“ Query Workflow**._
