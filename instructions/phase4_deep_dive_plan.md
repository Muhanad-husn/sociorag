# **SocioGraph RebuildÂ â€” PhaseÂ 4Â Deepâ€‘Dive Plan**

> **Objective:** Build a robust **query workflow** that detects the userâ€™s language, translates Arabic â†’â€¯English when needed, retrieves the most relevant vector chunks **and** graph triples, reranks with a crossâ€‘encoder, and merges everything into a trimmed context block ready for answer generation.

---

## ğŸ¯ Outcomes

| ID | Outcome | AcceptanceÂ Criteria |
|----|---------|---------------------|
| Oâ€‘4.1 | `normalize_query(text)` returns a tuple `(lang, text_en)` where `lang âˆˆ {"en","ar"}`. | Arabic input is translated to English; English remains unchanged. |
| Oâ€‘4.2 | `retrieve_chunks()` returns â‰¤â€¯`Config.TOP_K` texts whose cosineâ€¯â‰¥â€¯`Config.CHUNK_SIM` (0.85). | `len(chunks) â‰¤ 100` and all similarities logged. |
| Oâ€‘4.3 | `rerank_chunks()` keeps exactly `Config.TOP_K_RERANK` (15) highestâ€‘scoring docs using **crossâ€‘encoder/msâ€‘marcoâ€‘MiniLMâ€‘Lâ€‘6â€‘v2**. | Returned docs are ordered by descending score. îˆ€fileciteîˆ‚turn4file7îˆ |
| Oâ€‘4.4 | `retrieve_triples()` fetches graph triples whose entity similarityâ€¯â‰¥â€¯`Config.GRAPH_SIM`â€¯(0.95). | SQL ANN search proves at least one triple for noun entities. |
| Oâ€‘4.5 | `merge_context()` trims combined tokens to â‰¤â€¯40â€¯% of answerâ€‘model context window, preserving order *chunksÂ â†’Â triples*. | Token count verified via `tiktoken`. |
| Oâ€‘4.6 | Public API `retrieve_context(query: str) -> dict` returns `{chunks, triples, lang}` in <â€¯1.5â€¯s on laptop. | Pytest timer passes. |

---

## ğŸ“‹ Pipeline Overview  îˆ€fileciteîˆ‚turn4file2îˆ

```
Query
  â”‚
  â”œâ”€â–¶ LanguageÂ Detect (langdetect)
  â”‚     â””â”€â–¶ Arabic? â€”â–º Helsinki-NLP â‡¢ English  îˆ€fileciteîˆ‚turn4file17îˆ
  â”‚
  â”œâ”€â–¶ Embed query (Sentenceâ€‘Txf)
  â”‚
  â”œâ”€â–¶ VectorÂ Search (Chroma, simâ‰¥0.85, k=100)
  â”‚           â””â”€â–¶ Crossâ€‘Encoder Rerank âŸ topâ€‘15  îˆ€fileciteîˆ‚turn4file7îˆ
  â”‚
  â”œâ”€â–¶ Noun Extraction (spaCy)
  â”‚           â””â”€â–¶ ANN search on `entity` (simâ‰¥0.95) âŸ triples
  â”‚
  â””â”€â–¶ Merge & Trim (tiktoken, 40â€¯%)
                    â†“
           **Context Package**
```

---

## âš™ï¸ Prerequisites

```bash
pip install langdetect transformers tiktoken sentence-transformers
```

* PhasesÂ 0â€‘3 completed (env, singletons, ingest data present).  
* Set `OPENROUTER_API_KEY` for downstream phases (not strictly required here).  

---

## ğŸ› ï¸ Stepâ€‘byâ€‘Step Implementation

### 1Â Â ModuleÂ Layout

```
backend/app/retriever/
  __init__.py          # expose retrieve_context
  language.py          # detect & translate
  vector.py            # chunk retrieval + rerank
  graph.py             # nounâ†’triples
  merge.py             # tokenâ€‘aware merge
  pipeline.py          # orchestrator (retrieve_context)
```

Register tests under `backend/tests/test_retriever.py`.

---

### 2Â Â Language Detection & Translation (`language.py`)

```python
from langdetect import detect
from transformers import MarianMTModel, MarianTokenizer
from backend.app.core import LoggerSingleton

_tok, _model = None, None

def _load_helsinki():
    global _tok, _model
    if _tok is None:
        _tok = MarianTokenizer.from_pretrained("Helsinki-NLP/opus-mt-tc-big-ar-en")
        _model = MarianMTModel.from_pretrained("Helsinki-NLP/opus-mt-tc-big-ar-en")

def normalize_query(text: str) -> tuple[str, str]:
    lang = detect(text)
    if lang == "ar":
        _load_helsinki()
        inputs = _tok(text, return_tensors="pt")
        output = _model.generate(**inputs, max_length=256, num_beams=4, early_stopping=True)
        text_en = _tok.decode(output[0], skip_special_tokens=True)
        LoggerSingleton().info("Translated AR â†’ EN: %s", text_en)
        return "ar", text_en
    return "en", text
```

---

### 3Â Â Vector Retrieval & Crossâ€‘Encoder Rerank (`vector.py`)

```python
from langchain.vectorstores import Chroma
from sentence_transformers import CrossEncoder
from backend.app.core import ChromaSingleton, embed, LoggerSingleton, get_config

_cfg = get_config()
_reranker = CrossEncoder(_cfg.RERANKER_MODEL, max_length=512)

def retrieve_chunks(query_emb):
    vectordb = ChromaSingleton()
    docs = vectordb.similarity_search_by_vector(
        query_emb, k=_cfg.TOP_K, include=["embeddings", "metadatas"]
    )
    return docs

def rerank_chunks(query: str, docs):
    pairs = [(query, d.page_content) for d in docs]
    scores = _reranker.predict(pairs)
    ranked = sorted(zip(docs, scores), key=lambda t: t[1], reverse=True)
    return [d for d, _ in ranked[: _cfg.TOP_K_RERANK]]
```

---

### 4Â Â Graph Triples Retrieval (`graph.py`)

```python
from backend.app.core import NLPSingleton, SQLiteSingleton, embed, get_config
from array import array

_cfg = get_config()
_nlp = NLPSingleton()

def _fetch_entity_hits(noun: str):
    con = SQLiteSingleton()
    vec = embed(noun)[0]          # 384â€‘dim list
    sql = "SELECT id, surface, embedding FROM entity USING vss0 WHERE vss_search(embedding, ?) LIMIT 5"
    rows = con.execute(sql, (array("f", vec).tobytes(),)).fetchall()
    return [r for r in rows if cosine(r["embedding"], vec) >= _cfg.GRAPH_SIM]

def retrieve_triples(query_en: str):
    nouns = [t.text for t in _nlp(query_en) if t.pos_ == "NOUN"]
    con = SQLiteSingleton()
    triples = []
    for noun in nouns:
        for hit in _fetch_entity_hits(noun):
            triples.extend(con.execute(
                "SELECT * FROM relation WHERE head_id=? OR tail_id=?", (hit["id"], hit["id"])
            ).fetchall())
    return triples
```

---

### 5Â Â Context Merge & Token Budgeting (`merge.py`)

```python
import tiktoken
from backend.app.core import get_config

_enc = tiktoken.encoding_for_model("gpt-4o")         # safe default

def merge_context(chunks, triples, context_window=8192):
    texts = [d.page_content for d in chunks]
    triple_strs = [f"{t['head_id']} {t['rel_type']} {t['tail_id']}" for t in triples]
    combined = texts + triple_strs
    limit = int(context_window * get_config().MAX_CONTEXT_FRACTION)   # 0.4
    out = []
    tokens = 0
    for item in combined:
        t = len(_enc.encode(item))
        if tokens + t > limit:
            break
        out.append(item)
        tokens += t
    return out
```

---

### 6Â Â Orchestrator (`pipeline.py`)

```python
from backend.app.retriever.language import normalize_query
from backend.app.retriever.vector import retrieve_chunks, rerank_chunks
from backend.app.retriever.graph import retrieve_triples
from backend.app.retriever.merge import merge_context
from backend.app.core import embed

def retrieve_context(user_query: str):
    lang, query_en = normalize_query(user_query)
    q_vec = embed(query_en)[0]
    raw_chunks = retrieve_chunks(q_vec)
    chunks = rerank_chunks(query_en, raw_chunks)
    triples = retrieve_triples(query_en)
    context = merge_context(chunks, triples)
    return {"lang": lang, "chunks": chunks, "triples": triples, "context": context}
```

Expose `retrieve_context` in `backend/app/retriever/__init__.py`.

---

### 7Â Â Unit Tests (`test_retriever.py`)

```python
import time, pytest
from backend.app.retriever import retrieve_context

@pytest.mark.parametrize("q", ["Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…ØµØ±ØŸ", "What is social graph?"])
def test_retrieval(q):
    start = time.time()
    res = retrieve_context(q)
    assert "context" in res and len(res["context"]) > 0
    assert res["lang"] in {"en","ar"}
    assert time.time() - start < 1.5
```

Run `pytest -q`.

---

## ğŸ“ Validation Script

```bash
python - <<'PY'
from backend.app.retriever import retrieve_context
print(retrieve_context("Explain knowledge graphs in AI")["context"][:3])
PY
```

Expect first three context items printed.

---

## ğŸ•‘ Estimated Effort

| Task | Time (min) |
|------|------------|
| Language detect & translate | 10 |
| Vector retrieval & rerank | 15 |
| Graph triples | 15 |
| Merge + token budget | 5 |
| Orchestrator & tests | 10 |
| Docs & README | 5 |
| **Total** | **~1â€¯hr** |

---

## ğŸš‘ Troubleshooting & Tips

| Symptom | Likely Cause | Fix |
|---------|--------------|-----|
| Wrong language detected | `langdetect` heuristic weak for short text | Require â‰¥â€¯4 tokens before detection; else bypass translate |
| Rerank latency high | HF model on CPU | Batch size 16; reduce `TOP_K` |
| No triples returned | Nouns not found in graph | Lower `GRAPH_SIM` to 0.90 or fallback to fallback heuristics |
| Context too long | Large triples list | Drop leastâ€‘similar chunks before trimming triples |

---

## ğŸ“ Deliverables

1. **`backend/app/retriever/`** package with modules above.  
2. Public function **`retrieve_context`** returning merged context.  
3. Unit & validation tests passing.  
4. README â€œQuery Workflowâ€ section.  
5. Git tag **`phaseâ€‘4`**.

---

_When `retrieve_context()` returns a nonâ€‘empty context for both English and Arabic queries, and tests pass in <â€¯1.5â€¯s, **Phaseâ€¯4 is complete**. Next stop: **PhaseÂ 5Â â€“ Answer Generation & PDF Export**._

